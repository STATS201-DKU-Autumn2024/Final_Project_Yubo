{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Set file paths\n",
        "image_dir = '/content/drive/MyDrive/Stats201_FinalProject/Final_Project/images'\n",
        "train_path = '/content/drive/MyDrive/Stats201_FinalProject/Final_Project/train_filtered.csv'\n",
        "test_path = '/content/drive/MyDrive/Stats201_FinalProject/Final_Project/test_filtered.csv'\n",
        "val_path = '/content/drive/MyDrive/Stats201_FinalProject/Final_Project/val_filtered.csv'\n",
        "\n",
        "# Load the CSV files into pandas DataFrames\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "val_df = pd.read_csv(val_path)\n",
        "\n",
        "# Prepare image paths and labels\n",
        "train_image_paths = [os.path.join(image_dir, f\"{img_id}.jpg\") for img_id in train_df['ID']]\n",
        "test_image_paths = [os.path.join(image_dir, f\"{img_id}.jpg\") for img_id in test_df['ID']]\n",
        "val_image_paths = [os.path.join(image_dir, f\"{img_id}.jpg\") for img_id in val_df['ID']]\n",
        "\n",
        "train_labels = train_df['label'].values\n",
        "test_labels = test_df['label'].values\n",
        "val_labels = val_df['label'].values\n",
        "\n",
        "# Load the pre-trained ResNet50 model without the top layer for feature extraction\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Add global average pooling to reduce spatial dimensions\n",
        "x = Dense(1024, activation='relu')(x)  # Dense layer for non-linearity\n",
        "predictions = Dense(1, activation='sigmoid')(x)  # Sigmoid activation for binary classification\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model to prevent retraining\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# Create a tf.data.Dataset for the images and labels\n",
        "def create_dataset(image_paths, labels, batch_size=32):\n",
        "    # Create a tf.data.Dataset from the image paths and labels\n",
        "    image_paths_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    # Map the image loading and preprocessing function to the dataset\n",
        "    dataset = tf.data.Dataset.zip((image_paths_ds, labels_ds))\n",
        "    dataset = dataset.map(lambda img_path, label: (load_and_preprocess_image(img_path), label),\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Shuffle, batch, and prefetch to optimize the loading pipeline\n",
        "    dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Prepare datasets for training, validation, and testing\n",
        "train_dataset = create_dataset(train_image_paths, train_labels)\n",
        "val_dataset = create_dataset(val_image_paths, val_labels)\n",
        "test_dataset = create_dataset(test_image_paths, test_labels)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_preds = model.predict(test_dataset)\n",
        "test_preds = (test_preds > 0.5).astype(int)  # Convert probabilities to class labels\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f'Test Set Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(test_labels, test_preds, target_names=['Authentic', 'Machine-Generated']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbe7YVQZFiQp",
        "outputId": "ac94f97f-fa6d-45bb-8946-515fcb7d230e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 92ms/step - accuracy: 0.8762 - loss: 0.4137 - val_accuracy: 0.9595 - val_loss: 0.1025\n",
            "Epoch 2/5\n",
            "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 65ms/step - accuracy: 0.9684 - loss: 0.0834 - val_accuracy: 0.9702 - val_loss: 0.0820\n",
            "Epoch 3/5\n",
            "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 64ms/step - accuracy: 0.9752 - loss: 0.0604 - val_accuracy: 0.9665 - val_loss: 0.0879\n",
            "Epoch 4/5\n",
            "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 64ms/step - accuracy: 0.9810 - loss: 0.0469 - val_accuracy: 0.9709 - val_loss: 0.0900\n",
            "Epoch 5/5\n",
            "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 64ms/step - accuracy: 0.9925 - loss: 0.0224 - val_accuracy: 0.9738 - val_loss: 0.0926\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 72ms/step\n",
            "Test Set Accuracy: 50.37%\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "        Authentic       0.50      0.50      0.50      1328\n",
            "Machine-Generated       0.51      0.51      0.51      1372\n",
            "\n",
            "         accuracy                           0.50      2700\n",
            "        macro avg       0.50      0.50      0.50      2700\n",
            "     weighted avg       0.50      0.50      0.50      2700\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Xrf6l-aH4T-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}